# Transcription Service Setup

## Overview

The transcription service will use **faster-whisper** for high-performance audio transcription.

## Technology: faster-whisper

**Repository:** https://github.com/SYSTRAN/faster-whisper

**Why faster-whisper:**
- 4x faster than OpenAI's Whisper
- Lower memory usage
- Same accuracy
- CTranslate2 backend for optimization
- Supports GPU and CPU inference

## Supported Languages

Input languages (will be transcribed to English):
- Hindi
- Telugu
- English
- Tamil

## Architecture

```
┌─────────────────────────────────────────────┐
│  Transcription Service (Docker Container)   │
├─────────────────────────────────────────────┤
│                                             │
│  Input: /data/reels/{shortcode}/video.mp4  │
│                                             │
│  Step 1: FFmpeg Audio Extraction           │
│  ├─ Extract audio track                    │
│  ├─ Convert to WAV format                  │
│  └─ Save: audio.wav                        │
│                                             │
│  Step 2: faster-whisper Transcription      │
│  ├─ Load audio.wav                         │
│  ├─ Detect language                        │
│  ├─ Transcribe to English                  │
│  └─ Save: transcript.txt                   │
│                                             │
│  Output: transcript.txt + metadata.json    │
│                                             │
└─────────────────────────────────────────────┘
```

## Installation (Docker)

### Dockerfile Structure

```dockerfile
FROM python:3.11-slim

# Install FFmpeg
RUN apt-get update && apt-get install -y \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install faster-whisper
RUN pip install faster-whisper

WORKDIR /app

COPY . .

CMD ["python", "transcribe.py"]
```

## Model Selection

faster-whisper supports multiple model sizes:

| Model    | Parameters | VRAM   | Speed | Accuracy |
|----------|-----------|--------|-------|----------|
| tiny     | 39M       | ~1GB   | Fast  | Low      |
| base     | 74M       | ~1GB   | Fast  | Medium   |
| small    | 244M      | ~2GB   | Med   | Good     |
| medium   | 769M      | ~5GB   | Slow  | Better   |
| large-v2 | 1550M     | ~10GB  | Slow  | Best     |

**Recommended for local server:** `small` or `medium` model (good balance)

## API Endpoints (Planned)

### POST /transcribe

**Request:**
```json
{
  "shortcode": "DTQpr8DjlkU",
  "videoPath": "/data/reels/DTQpr8DjlkU/video.mp4"
}
```

**Response:**
```json
{
  "success": true,
  "shortcode": "DTQpr8DjlkU",
  "audioPath": "/data/reels/DTQpr8DjlkU/audio.wav",
  "transcriptPath": "/data/reels/DTQpr8DjlkU/transcript.txt",
  "transcript": "First, heat oil in a pan. Add cumin seeds...",
  "detectedLanguage": "hi",
  "duration": 45.3,
  "processingTime": 12.5
}
```

## Storage Contract

```
/data/reels/{shortcode}/
├── video.mp4          # Input (from Stage 1)
├── audio.wav          # Generated by FFmpeg
├── transcript.txt     # Generated by Whisper
└── metadata.json      # Updated with transcription info
```

## Implementation Steps

1. Create Python service with faster-whisper
2. Add FFmpeg audio extraction
3. Implement transcription endpoint
4. Add error handling and retries
5. Create Dockerfile
6. Add to docker-compose
7. Test with sample reels

## Performance Considerations

- Use GPU if available (CUDA support)
- Cache models on first load
- Process audio in chunks for long videos
- Implement queue for multiple requests
- Monitor memory usage

## Error Handling

- Invalid audio format → FFmpeg conversion
- Language not supported → Return error
- Transcription timeout → Retry with smaller model
- Storage issues → Log and alert

## Next Steps

1. Set up Python service structure
2. Install faster-whisper dependencies
3. Implement FFmpeg wrapper
4. Create transcription logic
5. Add API endpoints
6. Docker containerization
